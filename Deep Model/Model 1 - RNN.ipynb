{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision import transforms, utils\n",
    "import pickle as pkl\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from comet_ml import Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdb48132270>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manualSeed = 0\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('data/rnn/train-tokenized.txt', names=[\"previous\", \"current\", \"label\"])\n",
    "val_set = pd.read_csv('data/rnn/val-tokenized.txt', names=[\"previous\", \"current\", \"label\"])\n",
    "test_set = pd.read_csv('data/rnn/test-tokenized.txt', names=[\"previous\", \"current\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_balanced_set(dataset):\n",
    "    dataset_pos = dataset[dataset[\"label\"] == 1]\n",
    "    dataset_neg = dataset[dataset[\"label\"] == 0].sample(random_state = manualSeed, frac=1.).reset_index(drop=True)[:dataset_pos.shape[0]]\n",
    "    return pd.concat((dataset_pos, dataset_neg)).sample(random_state = manualSeed, frac=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced = to_balanced_set(train_set)#[:1000]\n",
    "val_balanced = to_balanced_set(val_set)#[:100]\n",
    "test_balanced = to_balanced_set(test_set)#[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful for\n",
    "# - debug\n",
    "# - choosing voc. size\n",
    "word_count = pkl.load(open('data/rnn/wordcount.pkl', 'r'))\n",
    "idx2word = pkl.load(open('data/rnn/idx2word.pkl', 'r'))\n",
    "word2idx = pkl.load(open('data/rnn/word2idx.pkl', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workers = 1\n",
    "batchSize = 50\n",
    "voc_size = 10000\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "model_type = \"GRU\" # \"RNN_TANH\", \"RNN_RELU\", \"GRU\"\n",
    "em_size = 10\n",
    "nhid = 200\n",
    "n_layers = 2\n",
    "dropout = .0\n",
    "bidirectional = True\n",
    "\n",
    "log_interval = 10\n",
    "\n",
    "lr = 1e-3\n",
    "clip = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    # As every elem in batch has different length, I pad all sentences in the batch with 0\n",
    "    # And sort them by length of sentences because of pack_padded_sequence, see pytorch doc.\n",
    "    \n",
    "    batch_size = len(batch)\n",
    "    lengths_sentence = []\n",
    "    labels_long = torch.LongTensor(batch_size)\n",
    "    labels = []\n",
    "    \n",
    "    for elem in batch:\n",
    "        lengths_sentence += [elem[0].size(0)]\n",
    "        labels += [elem[1][0]]\n",
    "        \n",
    "    lengths = torch.LongTensor(lengths_sentence)\n",
    "    max_length = max(lengths_sentence)    \n",
    "    lengths, indexes = torch.sort(lengths, dim=0, descending=True)\n",
    "    \n",
    "    x = torch.zeros(batch_size, max_length).long()\n",
    "    i = 0\n",
    "    \n",
    "    for idx in indexes:\n",
    "        x[i,:lengths[i]] = batch[idx][0]\n",
    "        labels_long[i] = labels[idx]\n",
    "        i += 1\n",
    "    \n",
    "    return x, lengths, labels_long\n",
    "\n",
    "class idx_to_sentence(object):\n",
    "    \n",
    "    def __init__(self, idx2word):\n",
    "        self.idx2word = idx2word\n",
    "        \n",
    "    def array_to_str(self, s_idx):\n",
    "        s_str = \"\"\n",
    "        for idx in s_idx:\n",
    "            s_str += self.idx2word[idx] + \" \"\n",
    "        return s_str\n",
    "        \n",
    "    def print_array(self, s_idx):\n",
    "        print(self.array_to_str(s_idx))\n",
    "    \n",
    "    \n",
    "class IronyDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, voc_size, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.voc_size = voc_size\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def idx_str_to_idx(self, idx):\n",
    "        idx = int(idx)\n",
    "        if idx >= voc_size:\n",
    "            # Return <UNK> token !\n",
    "            return 0\n",
    "        return idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        elem = self.dataset.iloc[idx]\n",
    "        prev, current, label = elem['previous'].split(\" \")[:-1], elem['current'].split(\" \")[1:-1], elem['label']\n",
    "        sentence = torch.LongTensor([self.idx_str_to_idx(i) for i in prev] + [self.idx_str_to_idx(i) for i in current])\n",
    "        label = torch.LongTensor([label])\n",
    "        return sentence, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_parser = idx_to_sentence(idx2word)\n",
    "\n",
    "train_wrapper = IronyDataset(train_balanced, voc_size)\n",
    "val_wrapper = IronyDataset(val_balanced, voc_size)\n",
    "test_wrapper = IronyDataset(test_balanced, voc_size)\n",
    "\n",
    "train_loader = DataLoader(train_wrapper, batch_size=batchSize, shuffle=True, num_workers=workers, collate_fn=collate)\n",
    "val_loader = DataLoader(val_wrapper, batch_size=batchSize, shuffle=True, num_workers=workers, collate_fn=collate)\n",
    "test_loader = DataLoader(test_wrapper, batch_size=batchSize, shuffle=True, num_workers=workers, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-14:\n",
      "Traceback (most recent call last):\n",
      "    self.run()\n",
      "  File \"/home/lucas/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/home/lucas/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lucas/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/lucas/anaconda2/lib/python2.7/multiprocessing/queues.py\", line 378, in get\n",
      "    return recv()\n",
      "  File \"/home/lucas/anaconda2/lib/python2.7/site-packages/torch/multiprocessing/queue.py\", line 21, in recv\n",
      "    buf = self.recv_bytes()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "a = iter(train_loader)\n",
    "text, length, label = a.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfollower = idx_to_sentence(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best news coming from Saudi since <UNK> <UNK> <eos> Good, now the women who run away because they aren't allowed to drive have a better chance of reaching safety instead of being thrown in prison for having too much of their face <UNK> <eos> \n"
     ]
    }
   ],
   "source": [
    "textfollower.print_array(text[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpad_input(input, length):\n",
    "    cat = []\n",
    "    batch_size = input.size(0)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        cat += [input[i,:length[i]]]\n",
    "\n",
    "    unpadded_input = torch.cat((cat), 0)\n",
    "    return unpadded_input\n",
    "\n",
    "class _attn(nn.Module):\n",
    "    def __init__(self, in_size, mid_size, heads):\n",
    "        super(_attn, self).__init__()\n",
    "        self.in_size = in_size\n",
    "        self.fc1 = nn.Linear(in_size, mid_size)\n",
    "        self.fc2 = nn.Linear(mid_size, heads)\n",
    "    \n",
    "    def forward(self, input, length):\n",
    "        # x : batch_size x max_len x in_size\n",
    "        # length : batch_size\n",
    "        x = unpad_input(input, length)\n",
    "        alphas = self.fc2(F.relu(self.fc1(x)))\n",
    "        \n",
    "        low_idx = 0\n",
    "        out = []\n",
    "        \n",
    "        idx = 0\n",
    "        for i in length:\n",
    "            high_idx = low_idx + i\n",
    "            out += [F.softmax(alphas[low_idx:high_idx]).transpose(0, 1).mm(input[idx,:length[idx]]).view(1, -1)]\n",
    "            idx += 1\n",
    "            low_idx += i\n",
    "        \n",
    "        return torch.cat((out), 0)\n",
    "    \n",
    "class _multihead_attn(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(_multihead_attn, self).__init__()\n",
    "    \n",
    "    def forward(self, x, length):\n",
    "        pass\n",
    "\n",
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, voc_size, in_size, mid_size, heads, dropout=0.5):\n",
    "        super(AttentionModel, self).__init__()\n",
    "        self.emb = nn.Embedding(voc_size, in_size)\n",
    "        self.attn1 = _attn(in_size, mid_size, heads)\n",
    "        self.fc1 = nn.Linear(in_size * heads, in_size)\n",
    "        self.attn2 = _attn(in_size, mid_size, heads)\n",
    "        self.fc2 = nn.Linear(in_size + in_size * heads, 1)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, length):\n",
    "        x = self.drop(self.emb(x))\n",
    "        out = self.attn1(x, length)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = torch.cat((out, self.attn2(x, length)), 1)\n",
    "        return F.sigmoid(self.fc2(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, bidirectional=True):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout, batch_first=False, bidirectional=bidirectional)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "            \n",
    "        decoder_in_size = nhid * nlayers # bidirectionnal\n",
    "        if bidirectional:\n",
    "            decoder_in_size *= 2\n",
    "        self.decoder = nn.Linear(decoder_in_size, 1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, length, hidden):\n",
    "        batch_size = input.size(0)\n",
    "        emb = self.encoder(input)\n",
    "        emb = self.drop(emb)\n",
    "        emb = pack_padded_sequence(emb, length, batch_first=True)\n",
    "        h_ts, h_T = self.rnn(emb, hidden)\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            (h_T, c_T) = h_t\n",
    "        h_T = h_T.view(batch_size, -1)\n",
    "        output = self.drop(h_T)\n",
    "        decoded = F.sigmoid(self.decoder(h_T))\n",
    "        return decoded\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        h_size = self.nlayers\n",
    "        if self.bidirectional:\n",
    "            h_size *= 2\n",
    "            \n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (Variable(weight.new(h_size, bsz, self.nhid).zero_()),\n",
    "                    Variable(weight.new(h_size, bsz, self.nhid).zero_()))\n",
    "        else:\n",
    "            return Variable(weight.new(h_size, bsz, self.nhid).zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionModel (\n",
      "  (emb): Embedding(100000, 100)\n",
      "  (attn1): _attn (\n",
      "    (fc1): Linear (100 -> 200)\n",
      "    (fc2): Linear (200 -> 10)\n",
      "  )\n",
      "  (fc1): Linear (1000 -> 100)\n",
      "  (attn2): _attn (\n",
      "    (fc1): Linear (100 -> 200)\n",
      "    (fc2): Linear (200 -> 10)\n",
      "  )\n",
      "  (fc2): Linear (1100 -> 1)\n",
      "  (drop): Dropout (p = 0.8)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AttentionModel(100000, 100, 200, 10, .8)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr = lr)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    # hidden = model.init_hidden(batchSize)\n",
    "    for batch_idx, (text, length, label) in enumerate(train_loader):\n",
    "        model.zero_grad()\n",
    "        batch_size = text.size(0)\n",
    "        # hidden = model.init_hidden(batch_size)\n",
    "        data = Variable(text)\n",
    "        label = Variable(label.float())\n",
    "        out = model(data, length.cpu().numpy()).squeeze()\n",
    "        loss = criterion(out, label)\n",
    "        total_loss += loss.data\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "#         for p in model.parameters():\n",
    "#             p.data.add_(-lr, p.grad.data)\n",
    "        \n",
    "        if batch_idx % log_interval == 0 and batch_idx > 0:\n",
    "            cur_loss = total_loss[0] / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:.6f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f}'.format(\n",
    "                epoch, batch_idx, len(train_loader), lr,\n",
    "                elapsed * 1000 / log_interval, cur_loss))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    # hidden = model.init_hidden(batchSize)\n",
    "    correct = 0\n",
    "    \n",
    "    fake_negative_printed = 0\n",
    "    fake_positive_printed = 0\n",
    "    \n",
    "    for batch_idx, (text, length, label) in enumerate(data_loader):\n",
    "        batch_size = text.size(0)\n",
    "        # hidden = model.init_hidden(batch_size)\n",
    "        data = Variable(text, volatile=True)\n",
    "        label = Variable(label.float(), volatile=True)\n",
    "        out = model(data, length.cpu().numpy())\n",
    "        loss = criterion(out, label).squeeze()\n",
    "        pred = out.data.round()\n",
    "        correct_batch = pred.eq(label.data.view_as(pred)).cpu()\n",
    "        correct += correct_batch.sum()\n",
    "        if correct_batch.sum() < batch_size and (fake_negative_printed < 5 or fake_positive_printed < 5):\n",
    "            for i in range(batch_size):\n",
    "                if correct_batch[i][0] == 0:\n",
    "                    if pred[i][0] == 1 and fake_positive_printed < 5:\n",
    "                        fake_positive_printed += 1\n",
    "                        textfollower.print_array(text[i,:length[i]].numpy())\n",
    "                        print(\"True : %d, Predicted : %d\" % (label[i].data[0], pred[i][0]))\n",
    "                    elif fake_negative_printed < 5:\n",
    "                        fake_negative_printed += 1\n",
    "                        textfollower.print_array(text[i,:length[i]].numpy())\n",
    "                        print(\"True : %d, Predicted : %d\" % (label[i].data[0], pred[i][0]))\n",
    "        \n",
    "        total_loss += loss.data\n",
    "    \n",
    "    total_loss /= len(data_loader)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {:.1f}/{:.0f} ({:.0f}%)\\n'.format(\n",
    "        total_loss[0], correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))\n",
    "    \n",
    "    return total_loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    10/ 3195 batches | lr 0.001000 | ms/batch 249.10 | loss  1.20\n",
      "| epoch   1 |    20/ 3195 batches | lr 0.001000 | ms/batch 237.16 | loss  0.82\n",
      "| epoch   1 |    30/ 3195 batches | lr 0.001000 | ms/batch 191.95 | loss  0.76\n",
      "| epoch   1 |    40/ 3195 batches | lr 0.001000 | ms/batch 196.03 | loss  0.81\n",
      "| epoch   1 |    50/ 3195 batches | lr 0.001000 | ms/batch 189.52 | loss  0.75\n",
      "| epoch   1 |    60/ 3195 batches | lr 0.001000 | ms/batch 210.92 | loss  0.77\n",
      "| epoch   1 |    70/ 3195 batches | lr 0.001000 | ms/batch 243.44 | loss  0.74\n",
      "| epoch   1 |    80/ 3195 batches | lr 0.001000 | ms/batch 258.69 | loss  0.72\n",
      "| epoch   1 |    90/ 3195 batches | lr 0.001000 | ms/batch 203.44 | loss  0.73\n",
      "| epoch   1 |   100/ 3195 batches | lr 0.001000 | ms/batch 184.07 | loss  0.74\n",
      "| epoch   1 |   110/ 3195 batches | lr 0.001000 | ms/batch 263.41 | loss  0.74\n",
      "| epoch   1 |   120/ 3195 batches | lr 0.001000 | ms/batch 322.44 | loss  0.72\n",
      "| epoch   1 |   130/ 3195 batches | lr 0.001000 | ms/batch 274.59 | loss  0.72\n",
      "| epoch   1 |   140/ 3195 batches | lr 0.001000 | ms/batch 195.31 | loss  0.76\n",
      "| epoch   1 |   150/ 3195 batches | lr 0.001000 | ms/batch 221.10 | loss  0.73\n",
      "| epoch   1 |   160/ 3195 batches | lr 0.001000 | ms/batch 175.09 | loss  0.73\n",
      "| epoch   1 |   170/ 3195 batches | lr 0.001000 | ms/batch 176.89 | loss  0.74\n",
      "| epoch   1 |   180/ 3195 batches | lr 0.001000 | ms/batch 180.92 | loss  0.73\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = np.inf\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(val_loader)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f}'.format(epoch, (time.time() - epoch_start_time), val_loss))\n",
    "    print('-' * 89)\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 1.5\n",
    "        optimizer = optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr /= 2\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canada should have a zone for refugees and any Canadian that is <UNK> should be required to live there. <eos> He doesn't have to be <UNK> <UNK> him 6 feet down from the surface of Canada is <UNK> <eos> \n",
      "True : 0, Predicted : 1\n",
      "In any of the GTA <UNK> if I <UNK> a car or shot at it, and they <UNK> off, I would chase them and finish them <UNK> <eos> <UNK> <UNK> <eos> \n",
      "True : 0, Predicted : 1\n",
      "If the economy is in good <UNK> then why hasn't the <UNK> raised interest rates yet? <eos> Are wages really the best way to <UNK> an <UNK> <UNK> <eos> \n",
      "True : 0, Predicted : 1\n",
      "As I've grown older I've realized that Santa likes rich kids more than everyone else <eos> Wow this is very <UNK> I have never heard this before. <eos> \n",
      "True : 1, Predicted : 0\n",
      "They ain't <UNK> if you own em <eos> those <UNK> are beautiful i want to cop but dont know where to find them <eos> \n",
      "True : 0, Predicted : 1\n",
      "<UNK> was a <UNK> on the <UNK> <UNK> <eos> Op doesnt know the basic <UNK> <UNK> this subreddit in a <UNK> i suppose <eos> \n",
      "True : 0, Predicted : 1\n",
      "It's impressive how much policy control Republicans have given up in their quest to never <UNK> <eos> I want to believe <eos> \n",
      "True : 0, Predicted : 1\n",
      "Is anyone else bothered when police tweet stuff like this? <eos> It's true they should have <UNK> her. <eos> \n",
      "True : 1, Predicted : 0\n",
      "Luke has to be including <UNK> as <UNK> <eos> Now that is how you do a trailer <eos> \n",
      "True : 0, Predicted : 1\n",
      "Take that <UNK> <eos> The San Diego <UNK> of Los Angeles has a nice ring to it. <eos> \n",
      "True : 1, Predicted : 0\n",
      "\n",
      "Test set: Average loss: 0.6841, Accuracy: 28970.0/50978 (57%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6841297149658203"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
